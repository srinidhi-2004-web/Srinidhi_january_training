Test-04
Supervized Machhine Learning Project

Project Title
Supervised Machine Learning Models for House Price Prediction

Problem Statement
The objective of this project is to build and compare five supervised machine learning models to predict house prices based on multiple features. The models are evaluated using standard regression metrics.

Dataset Description
Dataset: California Housing Dataset (from sklearn)

Features include:

MedInc: Median income
HouseAge: Average house age
AveRooms: Average rooms
AveBedrms: Average bedrooms
Population
AveOccup
Latitude
Longitude
Target Variable:

MedHouseVal (Median House Value)
The dataset contains more than 20,000 records and is suitable for regression.

Data Cleaning & Preprocessing Steps
1. Handling Missing Values
Checked for null values using .isnull().sum()
Filled missing values using median (robust to outliers)
2. Fixing Data Types
Ensured all numerical columns are of type float
Converted categorical variables (if any) to string
3. Outlier Detection and Treatment
Used IQR method
Capped extreme values
4. Removing Duplicates
Removed duplicate rows using drop_duplicates()
5. Encoding Categorical Variables
Used One-Hot Encoding via pd.get_dummies()
6. Feature Scaling
Applied StandardScaler for KNN and SVM
7. Removing Irrelevant Features
Removed highly correlated or constant features
8. Train-Test Split
Split data into 80% training and 20% testing
9. Skewness Treatment
Applied log transformation to skewed features
Algorithms Used
Linear Regression
Decision Tree Regressor
Random Forest Regressor
K-Nearest Neighbors Regressor
Support Vector Regressor (SVR)
Evaluation Metrics
RÂ² Score
Mean Squared Error (MSE)
Root Mean Squared Error (RMSE)
Mean Absolute Error (MAE)
Plot Comparison
plt.figure() plt.bar(results_df["Model"], results_df["R2"]) plt.title("R2 Score Comparison") plt.xlabel("Model") plt.ylabel("R2 Score") plt.show()

Sample Results (May Vary)
Model	R2	RMSE	MAE
Linear Regression	0.60	0.72	0.53
Decision Tree	0.64	0.69	0.49
Random Forest	0.82	0.48	0.32
KNN	0.70	0.62	0.44
SVM	0.75	0.58	0.40
Conclusion / Observations
Proper preprocessing improved model accuracy.
Random Forest performed best due to ensemble learning.
Linear Regression showed lowest performance due to linear assumptions.
Feature scaling was essential for KNN and SVM.
Tree-based models handled outliers better.
Overall, Random Forest is recommended for this dataset.
